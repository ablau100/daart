{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMDGM beta implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from daart.data import DataGenerator, compute_sequence_pad\n",
    "from daart.eval import get_precision_recall, run_lengths\n",
    "from daart.io import get_expt_dir, find_experiment\n",
    "from daart.transforms import ZScore\n",
    "\n",
    "from daart_utils.data import DataHandler\n",
    "from daart_utils.models import get_default_hparams\n",
    "from daart_utils.paths import data_path, results_path\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from scipy.special import softmax as scipy_softmax\n",
    "from scipy.stats import entropy\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from torch import nn, save\n",
    "\n",
    "from daart import losses\n",
    "from daart.models.base import BaseModel, reparameterize_gaussian, get_activation_func_from_str\n",
    "from daart.transforms import MakeOneHot\n",
    "    \n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions.relaxed_categorical import RelaxedOneHotCategorical\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.kl import kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'fly'\n",
    "input_type = 'markers'\n",
    "sequence_length = 2000\n",
    "batch_size = 12\n",
    "\n",
    "if dataset == 'fly':\n",
    "    from daart_utils.session_ids.fly import SESS_IDS_TRAIN_5, SESS_IDS_TEST\n",
    "    from daart_utils.session_ids.fly import label_names\n",
    "    sess_ids = SESS_IDS_TRAIN_5[0]\n",
    "\n",
    "output_size = len(label_names)\n",
    "# fill out hparams\n",
    "model_type = 'gmdgm'\n",
    "backbone = 'dtcn'\n",
    "hparams = get_default_hparams(\n",
    "    model_type=model_type, \n",
    "    backbone=backbone,\n",
    "    n_lags=4,\n",
    "    device='cuda', \n",
    "    batch_size=batch_size, \n",
    "    sequence_length=sequence_length,\n",
    "    input_type=input_type, output_size=output_size,\n",
    "    min_epochs=1, max_epochs=10, \n",
    "    enable_early_stop=False, early_stop_history=4, val_check_interval=4,\n",
    ")\n",
    "hparams['sequence_pad'] = compute_sequence_pad(hparams)\n",
    "hparams['data_dir'] = os.path.join(data_path, dataset)\n",
    "hparams['lambda_strong'] = 1\n",
    "hparams['lambda_pred'] = 0\n",
    "hparams['expt_ids'] = sess_ids\n",
    "hparams['expt_ids_to_keep'] = hparams['expt_ids']\n",
    "\n",
    "# we'll likely change these\n",
    "hparams['semi_supervised_algo'] = 'none'  # 'pseudo_labels' | 'ups' [todo]\n",
    "hparams['lambda_weak'] = 0\n",
    "hparams['anneal_start'] = 25\n",
    "hparams['anneal_end'] = 50\n",
    "\n",
    "#gmdgm params\n",
    "hparams['n_aug_classes'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Base models/modules in PyTorch.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GMDGM(BaseModel):\n",
    "    \"\"\"Gaussian Mixture Deep Generative Model.\n",
    "    \n",
    "    [insert arxiv link here]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        hparams : dict\n",
    "            - backbone (str): 'temporal-mlp' | 'dtcn' | 'lstm' | 'gru'\n",
    "            - rng_seed_model (int): random seed to control weight initialization\n",
    "            - input_size (int): number of input channels\n",
    "            - output_size (int): number of classes\n",
    "            - n_aug_classes (int): number of additional classes without labels\n",
    "            - sequence_pad (int): padding needed to account for convolutions\n",
    "            - n_hid_layers (int): hidden layers of network architecture\n",
    "            - n_hid_units (int): hidden units per layer\n",
    "            - n_lags (int): number of lags in input data to use for temporal convolution\n",
    "            - activation (str): 'linear' | 'relu' | 'lrelu' | 'sigmoid' | 'tanh'\n",
    "            - lambda_strong (float): hyperparam on strong label classification \n",
    "              (alpha in original paper)\n",
    "            \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        # model dict will contain some or all of the following components:\n",
    "        # - classifier: q(y|x) [weighted by hparams['lambda_strong'] on labeled data]\n",
    "        # - encoder: q(z|x,y)\n",
    "        # - decoder: p(x|z)\n",
    "        # - latent_generator: p(z|y)\n",
    "\n",
    "        self.model = nn.ModuleDict()\n",
    "        self.build_model()\n",
    "\n",
    "        # label loss based on cross entropy; don't compute gradient when target = 0\n",
    "        ignore_index = hparams.get('ignore_class', 0)\n",
    "        self.class_loss = nn.CrossEntropyLoss(ignore_index=ignore_index, reduction='mean')\n",
    "        # this will turn into a log-likelihood calculation using \\mu(z) as mean of normal\n",
    "        # self.reconstruction_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "    def graph_stuff(self, data):\n",
    "        predictions = self.predict_labels(data)['y_logits']\n",
    "        predictions_all.append(np.vstack(predictions[0]))\n",
    "        sns.set_context('talk')\n",
    "        sns.set_style('white')\n",
    "\n",
    "        idxs = (10000, 11000)\n",
    "        d = 0\n",
    "\n",
    "        for d, sess_id in enumerate(sess_ids):\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.plot(predictions_all[d][slice(*idxs)])\n",
    "            plt.ylabel('logits')\n",
    "            plt.xlabel('Time (bins)')\n",
    "            plt.title(sess_id)\n",
    "            plt.show()\n",
    "        \n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Construct the model using hparams.\"\"\"\n",
    "\n",
    "        # set random seeds for control over model initialization\n",
    "        rng_seed_model = self.hparams.get('rng_seed_model', 0)\n",
    "        torch.manual_seed(rng_seed_model)\n",
    "        np.random.seed(rng_seed_model)\n",
    "\n",
    "        # select backbone network\n",
    "        if self.hparams['backbone'].lower() == 'temporal-mlp':\n",
    "            from daart.backbones.temporalmlp import TemporalMLP as Module\n",
    "        elif self.hparams['backbone'].lower() == 'tcn':\n",
    "            raise NotImplementedError('deprecated; use dtcn instead')\n",
    "        elif self.hparams['backbone'].lower() == 'dtcn':\n",
    "            from daart.backbones.tcn import DilatedTCN as Module\n",
    "        elif self.hparams['backbone'].lower() in ['lstm', 'gru']:\n",
    "            from daart.backbones.rnn import RNN as Module\n",
    "        elif self.hparams['backbone'].lower() == 'tgm':\n",
    "            raise NotImplementedError\n",
    "            # from daart.models.tgm import TGM as Module\n",
    "        else:\n",
    "            raise ValueError('\"%s\" is not a valid backbone network' % self.hparams['backbone'])\n",
    "\n",
    "        n_total_classes = self.hparams['output_size'] + self.hparams['n_aug_classes']\n",
    "        self.hparams['n_total_classes'] = n_total_classes\n",
    "        # build label prior: p(y)\n",
    "        # prior prob for observed classes: 0.5 / n_observed_classes\n",
    "        # prior prob for unobserved classes: 0.5 / n_aug_classes\n",
    "        probs = 0.5 * np.ones((n_total_classes,))\n",
    "        probs[:self.hparams['output_size']] /= self.hparams['output_size']\n",
    "        probs[self.hparams['output_size']:] /= self.hparams['n_aug_classes']\n",
    "       \n",
    "        assert np.isclose([np.sum(probs)], [1])\n",
    "        #self.model['py'] = Categorical(torch.tensor(probs))      \n",
    "        \n",
    "        # build classifier: q(y|x)\n",
    "        self.model['qy_x'] = Module(\n",
    "            self.hparams, \n",
    "           # type='decoder',\n",
    "            in_size=self.hparams['input_size'], \n",
    "            hid_size=self.hparams['n_hid_units'], \n",
    "            out_size=n_total_classes)\n",
    "        \n",
    "        self.hparams['qy_x_temperature'] = 1\n",
    "        \n",
    "        # build encoder: q(z|x,y)\n",
    "        # for now we will concatenate x and y to infer z; perhaps in the future we\n",
    "        # can try a lookup table?\n",
    "        self.model['encoder'] = Module(\n",
    "            self.hparams, \n",
    "            in_size=n_total_classes + self.hparams['input_size'],\n",
    "            hid_size=self.hparams['n_hid_units'],\n",
    "            out_size=self.hparams['n_hid_units'])\n",
    "        self.model['qz_xy_mean'] = self._build_linear(\n",
    "            global_layer_num=len(self.model['qy_x'].model), name='qz_xy_mean',\n",
    "            in_size=self.hparams['n_hid_units'], out_size=self.hparams['n_hid_units'])\n",
    "        self.model['qz_xy_logvar'] = self._build_linear(\n",
    "            global_layer_num=len(self.model['qy_x'].model), name='qz_xy_logvar',\n",
    "            in_size=self.hparams['n_hid_units'], out_size=self.hparams['n_hid_units'])\n",
    "        \n",
    "        # build latent_generator: p(z|y)\n",
    "        # linear layer is essentially a lookup table of shape (n_hid_units, n_total_classes)\n",
    "        self.model['pz_y_mean'] = self._build_linear(\n",
    "            0, 'pz_y_mean', n_total_classes, self.hparams['n_hid_units'])\n",
    "        self.model['pz_y_logvar'] = self._build_linear(\n",
    "            0, 'pz_y_logvar', n_total_classes, self.hparams['n_hid_units'])\n",
    "        \n",
    "        # build decoder: p(x|z)\n",
    "        self.model['decoder'] = Module(\n",
    "            self.hparams, \n",
    "           # type='decoder',\n",
    "            in_size=self.hparams['n_hid_units'],\n",
    "            hid_size=self.hparams['n_hid_units'],\n",
    "            out_size=self.hparams['input_size'])\n",
    "        \n",
    "        self.hparams['kl_weight'] = 1  # weight in front of kl term; anneal this using callback\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"Process input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            observation data of shape (n_sequences, sequence_length, n_markers)\n",
    "        y : torch.Tensor\n",
    "            label data of shape (n_sequences, sequence_length)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict of model outputs/internals as torch tensors\n",
    "            - 'y_probs' (torch.Tensor): model classification\n",
    "               shape of (n_sequences, sequence_length, n_classes)\n",
    "            - 'y_sample' (torch.Tensor): sample from concrete distribution\n",
    "              shape of (n_sequences, sequence_length, n_classes)\n",
    "            - 'z_mean' (torch.Tensor): mean of appx posterior of latents in variational models\n",
    "              shape of (n_sequences, sequence_length, embedding_dim)\n",
    "            - 'z_logvar' (torch.Tensor): logvar of appx posterior of latents in variational models\n",
    "              shape of (n_sequences, sequence_length, embedding_dim)\n",
    "            - 'reconstruction' (torch.Tensor): input decoder prediction\n",
    "              shape of (n_sequences, sequence_length, n_markers)\n",
    "\n",
    "        \"\"\"\n",
    "        # push inputs through classifier to get q(y|x)\n",
    "        y_logits = self.model['qy_x'](x)\n",
    "        \n",
    "        # initialize and sample q(y|x) (should be a one-hot vector)\n",
    "        y_probs = nn.Softmax(dim=2)(y_logits)\n",
    "     \n",
    "        qy_x = RelaxedOneHotCategorical(\n",
    "            temperature=self.hparams['qy_x_temperature'], probs=y_probs)\n",
    "\n",
    "        \n",
    "        y_sample = qy_x.rsample()  # (n_sequences, sequence_length, n_total_classes)\n",
    "        \n",
    "        y_onehot = torch.zeros([y.shape[0], y.shape[1], self.hparams['n_total_classes']], device=y_logits.device)\n",
    "        for s in range(y.shape[0]):\n",
    "            one_hot = MakeOneHot()(y[s], self.hparams['n_total_classes'])\n",
    "            y_onehot[s] = one_hot\n",
    "\n",
    "        # init y_mixed, which will contain true labels for labeled data, samples for unlabled data\n",
    "        y_mixed = y_onehot.clone().detach()  # (n_sequences, sequence_length, n_total_classes)\n",
    "        # loop over sequences in batch\n",
    "        idxs_labeled = torch.zeros_like(y)\n",
    "        for s in range(y_mixed.shape[0]):\n",
    "            # for each sequence, update y_mixed with samples when true label is 0 \n",
    "            # (i.e. no label)\n",
    "            idxs_labeled[s] = y[s] != 0\n",
    "            y_mixed[s, ~idxs_labeled[s], :] = y_sample[s, ~idxs_labeled[s]]\n",
    "        \n",
    "        # concatenate sample with input x\n",
    "        # (n_sequences, sequence_length, n_total_classes))\n",
    "        xy = torch.cat([x, y_mixed], dim=2)\n",
    "        \n",
    "        # push y through generative model to get parameters of p(z|y)\n",
    "        pz_y_mean = self.model['pz_y_mean'](y_mixed)\n",
    "\n",
    "        pz_y_logvar = self.model['pz_y_logvar'](y_mixed)\n",
    "\n",
    "        \n",
    "        # push [y, x] through encoder to get parameters of q(z|x,y)\n",
    "        w = self.model['encoder'](xy)\n",
    "        qz_xy_mean = self.model['qz_xy_mean'](w)\n",
    "        qz_xy_logvar = self.model['qz_xy_logvar'](w)\n",
    "      \n",
    "      # sample with reparam trick\n",
    "        z_xy_sample = qz_xy_mean + torch.randn(qz_xy_mean.shape, device=y_logits.device) * qz_xy_logvar.exp().pow(0.5)\n",
    "\n",
    "        # push sampled z from through decoder to get reconstruction\n",
    "        # this will be the mean of p(x|z)\n",
    "        x_hat = self.model['decoder'](z_xy_sample)\n",
    "        \n",
    "        return {\n",
    "            'y_logits': y_logits, # (n_sequences, sequence_length, n_classes)\n",
    "            'y_probs': y_probs,  # (n_sequences, sequence_length, n_classes)\n",
    "            'y_sample': y_sample,  # (n_sequences, sequence_length, n_classes)\n",
    "            'qz_xy_mean': qz_xy_mean,  # (n_sequences, sequence_length, embedding_dim)\n",
    "            'qz_xy_logvar': qz_xy_logvar,  # (n_sequences, sequence_length, embedding_dim)\n",
    "            'pz_y_mean': pz_y_mean,  # (n_sequences, sequence_length, embedding_dim)\n",
    "            'pz_y_logvar': pz_y_logvar,  # (n_sequences, sequence_length, embedding_dim)\n",
    "            'reconstruction': x_hat,  # (n_sequences, sequence_length, n_markers)\n",
    "            'idxs_labeled': idxs_labeled,  # (n_sequences, sequence_length)\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def predict_labels(self, data_generator, return_scores=False, remove_pad=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_generator : DataGenerator object\n",
    "            data generator to serve data batches\n",
    "        return_scores : bool\n",
    "            return scores before they've been passed through softmax\n",
    "        remove_pad : bool\n",
    "            remove batch padding from model outputs before returning\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            - 'predictions' (list of lists): first list is over datasets; second list is over\n",
    "              batches in the dataset; each element is a numpy array of the label probability\n",
    "              distribution\n",
    "            - 'weak_labels' (list of lists): corresponding weak labels\n",
    "            - 'labels' (list of lists): corresponding labels\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "\n",
    "        pad = self.hparams.get('sequence_pad', 0)\n",
    "\n",
    "\n",
    "        # initialize outputs dict\n",
    "        keys = ['y_logits','y_probs','y_sample','qz_xy_mean','qz_xy_logvar'\n",
    "                ,'pz_y_mean','pz_y_logvar','reconstruction']\n",
    "        \n",
    "        results_dict = {}\n",
    "        for key in keys:\n",
    "            results_dict[key] = [[] for _ in range(data_generator.n_datasets)]\n",
    "\n",
    "            for sess, dataset in enumerate(data_generator.datasets):\n",
    "                results_dict[key][sess] = [np.array([]) for _ in range(dataset.n_sequences)]\n",
    "                \n",
    "\n",
    "        # partially fill container (gap trials will be included as nans)\n",
    "        dtypes = ['train', 'val', 'test']\n",
    "        for dtype in dtypes:\n",
    "            data_generator.reset_iterators(dtype)\n",
    "            for i in range(data_generator.n_tot_batches[dtype]):\n",
    "                data, sess_list = data_generator.next_batch(dtype)\n",
    "                outputs_dict = self.forward(data['markers'])\n",
    "                # remove padding if necessary\n",
    "                if pad > 0 and remove_pad:\n",
    "                    for key, val in outputs_dict.items():\n",
    "                        outputs_dict[key] = val[:, pad:-pad] if val is not None else None\n",
    "                # loop over sequences in batch\n",
    "                for s, sess in enumerate(sess_list):\n",
    "                    batch_idx = data['batch_idx'][s].item()\n",
    "                    for key in keys:\n",
    "                        \n",
    "                        # push through log-softmax, since this is included in the loss and not model\n",
    "                        results_dict[key][sess][batch_idx] = \\\n",
    "                            softmax(outputs_dict[key][s]).cpu().detach().numpy()\n",
    "                    \n",
    "        return results_dict\n",
    "    \n",
    "    def training_step(self, data, accumulate_grad=True, **kwargs):\n",
    "        \"\"\"Calculate negative log-likelihood loss for supervised models.\n",
    "        The batch is split into chunks if larger than a hard-coded `chunk_size` to keep memory\n",
    "        requirements low; gradients are accumulated across all chunks before a gradient step is\n",
    "        taken.\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dict\n",
    "            signals are of shape (n_sequences, sequence_length, n_channels)\n",
    "        accumulate_grad : bool, optional\n",
    "            accumulate gradient for training step\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            - 'loss' (float): total loss (negative log-like under specified noise dist)\n",
    "            - other loss terms depending on model hyperparameters\n",
    "        \"\"\"\n",
    "        # graph stuff (logits) llll\n",
    "        self.graph_stuff(data)\n",
    "        \n",
    "        \n",
    "        # define hyperparams\n",
    "        lambda_strong = self.hparams.get('lambda_strong', 1)\n",
    "        kl_weight = self.hparams.get('kl_weight', 1)\n",
    "\n",
    "        # index padding for convolutions\n",
    "        pad = self.hparams.get('sequence_pad', 0)\n",
    "\n",
    "        # push data through model\n",
    "        markers_wpad = data['markers']\n",
    "        labels_wpad = data['labels_strong']\n",
    "        outputs_dict = self.forward(markers_wpad, labels_wpad)\n",
    "\n",
    "        # remove padding from supplied data\n",
    "        if pad > 0:\n",
    "            labels_strong = data['labels_strong'][:, pad:-pad, ...]\n",
    "        else:\n",
    "            labels_strong = data['labels_strong']\n",
    "        \n",
    "        # remove padding from model output\n",
    "        if pad > 0:\n",
    "            markers = markers_wpad[:, pad:-pad, ...]\n",
    "            # remove padding from model output\n",
    "            for key, val in outputs_dict.items():\n",
    "                outputs_dict[key] = val[:, pad:-pad, ...] if val is not None else None\n",
    "        else:\n",
    "            markers = markers_wpad\n",
    "            \n",
    "        # reshape everything to be (n_sequences * sequence_length, ...)\n",
    "        N = markers.shape[0] * markers.shape[1]\n",
    "        markers_rs = torch.reshape(markers, (N, markers.shape[-1]))\n",
    "        labels_rs = torch.reshape(labels_strong, (N,))\n",
    "        outputs_dict_rs = {}\n",
    "        for key, val in outputs_dict.items():\n",
    "            if isinstance(val, torch.Tensor):\n",
    "                if len(val.shape) > 2:\n",
    "                    outputs_dict_rs[key] = torch.reshape(val, (N, val.shape[-1]))\n",
    "                else:\n",
    "                    # when the input is (n_sequences, sequence_length), we want the output to be (n_sequences * sequence_length)\n",
    "                    outputs_dict_rs[key] = torch.reshape(val, (N, 1))\n",
    "            else:\n",
    "                outputs_dict_rs[key] = val\n",
    "                \n",
    "        # pull out indices of labeled data for loss computation\n",
    "        idxs_labeled = outputs_dict_rs['idxs_labeled']\n",
    "\n",
    "        # initialize loss to zero\n",
    "        loss = 0\n",
    "        loss_dict = {}\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # compute classification loss on labeled data\n",
    "        # ----------------------------------------------\n",
    "        if lambda_strong > 0:\n",
    "            loss_strong = self.class_loss(outputs_dict_rs['y_logits'], labels_rs)\n",
    "            loss += lambda_strong * loss_strong\n",
    "            # log\n",
    "            loss_dict['loss_classifier'] = loss_strong.item()\n",
    "           # print('gt shape', labels_rs.shape)\n",
    "            #print(labels_rs[:5])\n",
    "            #print('pred shape', outputs_dict_rs['y_logits'].shape)\n",
    "           # print(outputs_dict_rs['y_logits'][:5])\n",
    "            print('loss classifier: ', loss_strong.item())\n",
    "            \n",
    "        # ------------------------------------\n",
    "        # compute reconstruction loss\n",
    "        # ------------------------------------ \n",
    "        reconstruction = outputs_dict_rs['reconstruction']\n",
    "        px_z_mean = reconstruction\n",
    "        px_z_std = torch.ones_like(px_z_mean)\n",
    "\n",
    "        px_z = Normal(px_z_mean, px_z_std)\n",
    "        \n",
    "        # diff bwetween log prob of adding 1d Normals and MVN log prob\n",
    "        k = markers_rs.shape[1]\n",
    "        mvn_scalar = (k**2 * (2*np.pi)**((k-1)/2))**(-1)\n",
    "        # take sum over latent dim\n",
    "        loss_reconstruction = torch.sum(px_z.log_prob(markers_rs), axis=1) * mvn_scalar\n",
    "        # average over batch dim\n",
    "        loss_reconstruction = torch.mean(loss_reconstruction, axis=0)\n",
    "        \n",
    "        loss += loss_reconstruction\n",
    "        # log\n",
    "        loss_dict['loss_reconstruction'] = loss_reconstruction.item()\n",
    "       # print('loss recon: ', loss_reconstruction)\n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # compute kl divergence b/t qz_xy and pz_y\n",
    "        # ----------------------------------------   \n",
    "        # build MVN p(z|y)\n",
    "        pz_y_mean = outputs_dict_rs['pz_y_mean']\n",
    "        pz_y_std = outputs_dict_rs['pz_y_logvar'].exp().pow(0.5)\n",
    "        pz_y = Normal(pz_y_mean, pz_y_std)\n",
    "        \n",
    "        # build MVN q(z|x,y)\n",
    "        qz_xy_mean = outputs_dict_rs['qz_xy_mean']\n",
    "        qz_xy_std = outputs_dict_rs['qz_xy_logvar'].exp().pow(0.5)\n",
    "        qz_xy = Normal(qz_xy_mean, qz_xy_std)\n",
    "        \n",
    "        # sum over latent, mean over batch\n",
    "        loss_kl = torch.mean(torch.sum(kl_divergence(qz_xy, pz_y), axis=1), axis=0)\n",
    "        \n",
    "        loss -= kl_weight * loss_kl\n",
    "        # log\n",
    "        loss_dict['kl_weight'] = kl_weight\n",
    "        loss_dict['loss_kl'] = loss_kl.item()\n",
    "      #  print('KL weight: ', kl_weight)\n",
    "       # print('loss KL (w/o weight): ', loss_kl)\n",
    "            \n",
    "        # ---------------------------------------\n",
    "        # entropy loss of qy_x on unlabeled data\n",
    "        # ---------------------------------------\n",
    "        loss_unlabeled = 0\n",
    "        y_probs = outputs_dict_rs['y_probs'][~idxs_labeled]\n",
    "        loss_entropy = Categorical(y_probs).entropy()\n",
    "       # print('loss e shape: ', loss_entropy.shape)\n",
    "        \n",
    "        loss_unlabeled -= torch.mean(loss_entropy, axis=0)\n",
    "       # print('entropy loss: ', loss_unlabeled)\n",
    "        \n",
    "        # log\n",
    "        loss_dict['loss_unlabeled'] = loss_unlabeled.item()\n",
    "        \n",
    "        # adding unlabeled loss to total loss\n",
    "        loss += loss_unlabeled[0]\n",
    "        loss = loss * (-1) # note minimize -elbo\n",
    "       # print('TOTAL LOSS: ', loss)\n",
    "        if accumulate_grad:\n",
    "            loss.backward()\n",
    "\n",
    "        # collect loss vals\n",
    "        loss_dict['loss'] = loss.item()\n",
    "\n",
    "        return loss_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Generator contains 5 SingleDataset objects:\n",
      "2019_08_07_fly2\n",
      "    signals: ['markers', 'labels_strong']\n",
      "    transforms: OrderedDict([('markers', ZScore()), ('labels_strong', None)])\n",
      "    paths: OrderedDict([('markers', '/home/bsb2144/daart/daart_utils/data/fly/markers/2019_08_07_fly2_labeled.h5'), ('labels_strong', '/home/bsb2144/daart/daart_utils/data/fly/labels-hand/2019_08_07_fly2_labels.csv')])\n",
      "2019_08_08_fly1\n",
      "    signals: ['markers', 'labels_strong']\n",
      "    transforms: OrderedDict([('markers', ZScore()), ('labels_strong', None)])\n",
      "    paths: OrderedDict([('markers', '/home/bsb2144/daart/daart_utils/data/fly/markers/2019_08_08_fly1_labeled.h5'), ('labels_strong', '/home/bsb2144/daart/daart_utils/data/fly/labels-hand/2019_08_08_fly1_labels.csv')])\n",
      "2019_08_20_fly2\n",
      "    signals: ['markers', 'labels_strong']\n",
      "    transforms: OrderedDict([('markers', ZScore()), ('labels_strong', None)])\n",
      "    paths: OrderedDict([('markers', '/home/bsb2144/daart/daart_utils/data/fly/markers/2019_08_20_fly2_labeled.h5'), ('labels_strong', '/home/bsb2144/daart/daart_utils/data/fly/labels-hand/2019_08_20_fly2_labels.csv')])\n",
      "2019_10_10_fly3\n",
      "    signals: ['markers', 'labels_strong']\n",
      "    transforms: OrderedDict([('markers', ZScore()), ('labels_strong', None)])\n",
      "    paths: OrderedDict([('markers', '/home/bsb2144/daart/daart_utils/data/fly/markers/2019_10_10_fly3_labeled.h5'), ('labels_strong', '/home/bsb2144/daart/daart_utils/data/fly/labels-hand/2019_10_10_fly3_labels.csv')])\n",
      "2019_10_14_fly3\n",
      "    signals: ['markers', 'labels_strong']\n",
      "    transforms: OrderedDict([('markers', ZScore()), ('labels_strong', None)])\n",
      "    paths: OrderedDict([('markers', '/home/bsb2144/daart/daart_utils/data/fly/markers/2019_10_14_fly3_labeled.h5'), ('labels_strong', '/home/bsb2144/daart/daart_utils/data/fly/labels-hand/2019_10_14_fly3_labels.csv')])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# build data generator\n",
    "# -------------------------------------\n",
    "signals = []\n",
    "transforms = []\n",
    "paths = []\n",
    "\n",
    "handlers = []\n",
    "heuristics = []\n",
    "for expt_id in hparams['expt_ids']:\n",
    "\n",
    "    signals_curr = []\n",
    "    transforms_curr = []\n",
    "    paths_curr = []\n",
    "\n",
    "    # DLC markers or features (i.e. from simba)\n",
    "    input_type = hparams.get('input_type', 'markers')\n",
    "    markers_file = os.path.join(hparams['data_dir'], input_type, expt_id + '_labeled.h5')\n",
    "    if not os.path.exists(markers_file):\n",
    "        markers_file = os.path.join(hparams['data_dir'], input_type, expt_id + '_labeled.csv')\n",
    "    if not os.path.exists(markers_file):\n",
    "        markers_file = os.path.join(hparams['data_dir'], input_type, expt_id + '_labeled.npy')\n",
    "    if not os.path.exists(markers_file):\n",
    "        raise FileNotFoundError('could not find marker file for %s' % expt_id)\n",
    "    signals_curr.append('markers')\n",
    "    transforms_curr.append(ZScore())\n",
    "    paths_curr.append(markers_file)\n",
    "\n",
    "    # heuristic labels\n",
    "    if hparams.get('lambda_weak', 0) > 0:\n",
    "        heur_labels_file = os.path.join(\n",
    "            hparams['data_dir'], 'labels-heuristic', expt_id + '_labels.csv')\n",
    "        signals_curr.append('labels_weak')\n",
    "        transforms_curr.append(None)\n",
    "        paths_curr.append(heur_labels_file)\n",
    "\n",
    "    # hand labels\n",
    "    if hparams.get('lambda_strong', 0) > 0:\n",
    "        if expt_id not in hparams['expt_ids_to_keep']:\n",
    "            hand_labels_file = None\n",
    "        else:\n",
    "            hand_labels_file = os.path.join(\n",
    "                hparams['data_dir'], 'labels-hand', expt_id + '_labels.csv')\n",
    "            if not os.path.exists(hand_labels_file):\n",
    "                hand_labels_file = None\n",
    "        signals_curr.append('labels_strong')\n",
    "        transforms_curr.append(None)\n",
    "        paths_curr.append(hand_labels_file)\n",
    "\n",
    "    # define data generator signals\n",
    "    signals.append(signals_curr)\n",
    "    transforms.append(transforms_curr)\n",
    "    paths.append(paths_curr)\n",
    "    \n",
    "    # load data handler\n",
    "    handler = DataHandler(expt_id, base_path=os.path.join(hparams['data_dir']))\n",
    "    handler.load_hand_labels()\n",
    "    states = np.argmax(handler.hand_labels.vals, axis=1)\n",
    "    cutoff = int(np.floor(states.shape[0] / hparams['batch_size'])) * hparams['batch_size']\n",
    "    handler.load_heuristic_labels()\n",
    "    states_h = np.argmax(handler.heuristic_labels.vals, axis=1)\n",
    "    states_heuristic = states_h[:cutoff]   \n",
    "    handlers.append(handler)\n",
    "    heuristics.append(heuristics)\n",
    "\n",
    "# build data generator\n",
    "print('Loading data...')\n",
    "data_generator = DataGenerator(\n",
    "    hparams['expt_ids'], signals, transforms, paths, device=hparams['device'],\n",
    "    batch_size=hparams['batch_size'], sequence_length=hparams['sequence_length'],\n",
    "    trial_splits=hparams['trial_splits'],\n",
    "    train_frac=hparams['train_frac'], sequence_pad=hparams['sequence_pad'],\n",
    "    input_type=hparams.get('input_type', 'markers'))\n",
    "print(data_generator)\n",
    "\n",
    "# automatically compute input/output sizes from data\n",
    "hparams['input_size'] = data_generator.datasets[0].data['markers'][0].shape[1]\n",
    "\n",
    "# -------------------------------------\n",
    "# build model\n",
    "# -------------------------------------\n",
    "hparams['rng_seed_model'] = hparams['rng_seed_train']  # TODO: get rid of this\n",
    "torch.manual_seed(hparams.get('rng_seed_model', 0))\n",
    "if hparams['model_type'] == 'segmenter':\n",
    "    model = Segmenter(hparams)\n",
    "elif hparams['model_type'] == 'gmdgm':\n",
    "    model = GMDGM(hparams)\n",
    "model.to(hparams['device'])\n",
    "#print(model)\n",
    "\n",
    "# -------------------------------------\n",
    "# set up training callbacks\n",
    "# -------------------------------------\n",
    "callbacks = []\n",
    "if hparams.get('semi_supervised_algo', 'none') == 'pseudo_labels':\n",
    "    from daart.callbacks import AnnealHparam, PseudoLabels\n",
    "    if model.hparams['lambda_weak'] == 0:\n",
    "        print('warning! use lambda_weak in model.yaml to weight pseudo label loss')\n",
    "    else:\n",
    "        callbacks.append(AnnealHparam(\n",
    "            hparams=model.hparams, key='lambda_weak', epoch_start=hparams['anneal_start'],\n",
    "            epoch_end=hparams['anneal_end']))\n",
    "        callbacks.append(PseudoLabels(\n",
    "            prob_threshold=hparams['prob_threshold'], epoch_start=hparams['anneal_start']))\n",
    "        # set min_epochs to when annealings ends\n",
    "        hparams['min_epochs'] = hparams['anneal_end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'n_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9cb48b6c42ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulate_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_epoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f5c69124c84b>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, data, accumulate_grad, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \"\"\"\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# graph stuff (logits) llll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_stuff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f5c69124c84b>\u001b[0m in \u001b[0;36mgraph_stuff\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgraph_stuff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mpredictions_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'talk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f5c69124c84b>\u001b[0m in \u001b[0;36mpredict_labels\u001b[0;34m(self, data_generator, return_scores, remove_pad)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mresults_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mresults_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_datasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'n_datasets'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from daart.train import Logger, Trainer\n",
    "\n",
    "# -----------------------------------\n",
    "# set up training\n",
    "# -----------------------------------\n",
    "\n",
    "# dummy class to feed to callbacks (keeps track of batch/epoch number)\n",
    "trainer = Trainer(**hparams, callbacks=callbacks)\n",
    "\n",
    "# optimizer setup\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.get_parameters(), lr=hparams['learning_rate'], weight_decay=hparams['l2_reg'], amsgrad=True)\n",
    "\n",
    "# set random seeds for training\n",
    "torch.manual_seed(hparams['rng_seed_train'])\n",
    "np.random.seed(hparams['rng_seed_train'])\n",
    "\n",
    "# -----------------------------------\n",
    "# train loop\n",
    "# -----------------------------------\n",
    "\n",
    "best_val_epoch = None\n",
    "for i_epoch in tqdm(range(hparams['max_epochs'] + 1)):\n",
    "    # Note: the 0th epoch has no training (randomly initialized model is evaluated) so we\n",
    "    # cycle through `max_epochs` training epochs\n",
    "    trainer.curr_epoch = i_epoch\n",
    "\n",
    "    # control how data is batched to that models can be restarted from a particular epoch\n",
    "    torch.manual_seed(hparams['rng_seed_train'] + i_epoch)  # order of batches within datasets\n",
    "    np.random.seed(hparams['rng_seed_train'] + i_epoch)  # order of datasets\n",
    "\n",
    "    data_generator.reset_iterators('train')\n",
    "\n",
    "    i_batch = 0\n",
    "    for i_batch in range(data_generator.n_tot_batches['train']):\n",
    "\n",
    "        if i_epoch > 0:\n",
    "            trainer.curr_batch += 1\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        data, dataset = data_generator.next_batch('train')\n",
    "        loss_dict = model.training_step(data, dataset=dataset, accumulate_grad=True)\n",
    "        if i_epoch > 0:\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# from daart.train import Logger, Trainer\n",
    "\n",
    "# # define training params\n",
    "# train_kwargs = {\n",
    "#     'learning_rate': 1e-4,       # adam learning rate\n",
    "#     'l2_reg': 0,                 # general l2 reg on parameters\n",
    "#     'min_epochs': 10,            # minimum number of training epochs\n",
    "#     'max_epochs': 200,          # maximum number of training epochs\n",
    "#     'val_check_interval': 1,     # requency with which to log performance on val data\n",
    "#     'rng_seed_train': 0,         # control order in which data are served to model\n",
    "#     'enable_early_stop': False,  # True to use early stopping; False will use max_epochs\n",
    "#     'early_stop_history': 10,    # epochs over which to average early stopping metric\n",
    "#     'save_last_model': False,    # true to save out last (as well as best) model\n",
    "# }\n",
    "\n",
    "# # fit model!\n",
    "# # Trainer(data_gen, save_path=model_save_path, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
