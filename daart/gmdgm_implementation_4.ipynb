{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMDGM beta implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from daart.data import DataGenerator, compute_sequence_pad\n",
    "from daart.eval import get_precision_recall, run_lengths\n",
    "from daart.io import get_expt_dir, find_experiment\n",
    "from daart.models import Segmenter\n",
    "from daart.transforms import ZScore\n",
    "\n",
    "from daart_utils.data import DataHandler\n",
    "from daart_utils.models import get_default_hparams\n",
    "from daart_utils.paths import data_path, results_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'fly'\n",
    "input_type = 'markers'\n",
    "sequence_length = 2000\n",
    "batch_size = 12\n",
    "\n",
    "if dataset == 'fly':\n",
    "    from daart_utils.session_ids.fly import SESS_IDS_TRAIN_5, SESS_IDS_TEST\n",
    "    from daart_utils.session_ids.fly import label_names\n",
    "    sess_ids = SESS_IDS_TRAIN_5[0]\n",
    "\n",
    "output_size = len(label_names)\n",
    "# fill out hparams\n",
    "model_type = 'gmdgm'\n",
    "backbone = 'dtcn'\n",
    "hparams = get_default_hparams(\n",
    "    model_type=model_type, \n",
    "    backbone=backbone,\n",
    "    n_lags=4,\n",
    "    device='cuda', \n",
    "    batch_size=batch_size, \n",
    "    sequence_length=sequence_length,\n",
    "    input_type=input_type, output_size=output_size,\n",
    "    min_epochs=1, max_epochs=2, \n",
    "    enable_early_stop=False, early_stop_history=4, val_check_interval=4,\n",
    ")\n",
    "hparams['sequence_pad'] = compute_sequence_pad(hparams)\n",
    "hparams['data_dir'] = os.path.join(data_path, dataset)\n",
    "hparams['lambda_strong'] = 1\n",
    "hparams['lambda_pred'] = 0\n",
    "hparams['expt_ids'] = sess_ids\n",
    "hparams['expt_ids_to_keep'] = hparams['expt_ids']\n",
    "\n",
    "# we'll likely change these\n",
    "hparams['semi_supervised_algo'] = 'none'  # 'pseudo_labels' | 'ups' [todo]\n",
    "hparams['lambda_weak'] = 0\n",
    "hparams['anneal_start'] = 25\n",
    "hparams['anneal_end'] = 75\n",
    "hparams['prob_threshold'] = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'losses'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d815d3edad20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdaart\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdaart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'losses'"
     ]
    }
   ],
   "source": [
    "\"\"\"Base models/modules in PyTorch.\"\"\"\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from scipy.special import softmax as scipy_softmax\n",
    "from scipy.stats import entropy\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from torch import nn, save\n",
    "\n",
    "from daart import losses\n",
    "from daart.models.base import BaseModel\n",
    "    \n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.distributions.relaxed_categorical import RelaxedOneHotCategorical\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.kl import kl_divergence\n",
    "\n",
    "class GMDGM(BaseModel):\n",
    "    \"\"\"Gaussian Mixture Deep Generative Model.\n",
    "    \n",
    "    [insert arxiv link here]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        hparams : dict\n",
    "            - backbone (str): 'temporal-mlp' | 'dtcn' | 'lstm' | 'gru'\n",
    "            - rng_seed_model (int): random seed to control weight initialization\n",
    "            - input_size (int): number of input channels\n",
    "            - output_size (int): number of classes\n",
    "            - n_aug_classes (int): number of additional classes without labels\n",
    "            - sequence_pad (int): padding needed to account for convolutions\n",
    "            - n_hid_layers (int): hidden layers of network architecture\n",
    "            - n_hid_units (int): hidden units per layer\n",
    "            - n_lags (int): number of lags in input data to use for temporal convolution\n",
    "            - activation (str): 'linear' | 'relu' | 'lrelu' | 'sigmoid' | 'tanh'\n",
    "            - lambda_strong (float): hyperparam on strong label classification \n",
    "              (alpha in original paper)\n",
    "            \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        # model dict will contain some or all of the following components:\n",
    "        # - classifier: q(y|x) [weighted by hparams['lambda_strong'] on labeled data]\n",
    "        # - encoder: q(z|x,y)\n",
    "        # - decoder: p(x|z)\n",
    "        # - latent_generator: p(z|y)\n",
    "\n",
    "        self.model = nn.ModuleDict()\n",
    "        self.build_model()\n",
    "\n",
    "        # label loss based on cross entropy; don't compute gradient when target = 0\n",
    "        ignore_index = hparams.get('ignore_class', 0)\n",
    "        self.class_loss = nn.CrossEntropyLoss(ignore_index=ignore_index, reduction='mean')\n",
    "        # this will turn into a log-likelihood calculation using \\mu(z) as mean of normal\n",
    "        # self.reconstruction_loss = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"Pretty print model architecture.\"\"\"\n",
    "\n",
    "        # list: encoder, decoder, qz_mean, qz_logvar, classifier,  \n",
    "        \n",
    "        # py, qy_x, encoder--, qz_xy_mean, qz_xy_logvar, 'pz_y_mean, 'pz_y_logvar, decoder--\n",
    "        \n",
    "        format_str = '\\n%s architecture\\n' % self.hparams['backbone'].upper()\n",
    "        format_str += '------------------------\\n'\n",
    "\n",
    "        format_str += 'Encoder:\\n'\n",
    "        for i, module in enumerate(self.model['encoder'].model):\n",
    "            format_str += str('    {}: {}\\n'.format(i, module))\n",
    "        format_str += '\\n'\n",
    "\n",
    "        if 'decoder' in self.model:\n",
    "            format_str += 'Decoder:\\n'\n",
    "            for i, module in enumerate(self.model['decoder'].model):\n",
    "                format_str += str('    {}: {}\\n'.format(i, module))\n",
    "            format_str += '\\n'\n",
    "\n",
    "        if 'py' in self.model:\n",
    "            format_str += 'p(y):\\n'\n",
    "            for i, module in enumerate(self.model['py']):\n",
    "                format_str += str('    {}: {}\\n'.format(i, module))\n",
    "            format_str += '\\n'\n",
    "\n",
    "        if 'qy_x' in self.model:\n",
    "            format_str += 'q(y|x):\\n'\n",
    "            for i, module in enumerate(self.model['qy_x']):\n",
    "                format_str += str('    {}: {}\\n'.format(i, module))\n",
    "            format_str += '\\n'\n",
    "\n",
    "        if 'qz_xy_mean' in self.model:\n",
    "            format_str += 'q(z|xy) mean:\\n'\n",
    "            for i, module in enumerate(self.model['qz_xy_mean']):\n",
    "                format_str += str('    {}: {}\\n'.format(i, module))\n",
    "                \n",
    "        if 'qz_xy_logvar' in self.model:\n",
    "            format_str += 'q(z|xy) logvar:\\n'\n",
    "            for i, module in enumerate(self.model['qz_xy_logvar']):\n",
    "                format_str += str('    {}: {}\\n'.format(i, module))\n",
    "                \n",
    "        if 'pz_y_mean' in self.model:\n",
    "            format_str += 'p(z|y) mean:\\n'\n",
    "            for i, module in enumerate(self.model['pz_y_mean']):\n",
    "                format_str += str('    {}: {}\\n'.format(i, module))\n",
    "                \n",
    "        if 'pz_y_logvar' in self.model:\n",
    "            format_str += 'p(z|y) logvar:\\n'\n",
    "            for i, module in enumerate(self.model['pz_y_logvar']):\n",
    "                format_str += str('    {}: {}\\n'.format(i, module))\n",
    "\n",
    "        return format_str\n",
    "    \n",
    "    \n",
    "    def predict_labels(self, data_generator, return_scores=False, remove_pad=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_generator : DataGenerator object\n",
    "            data generator to serve data batches\n",
    "        return_scores : bool\n",
    "            return scores before they've been passed through softmax\n",
    "        remove_pad : bool\n",
    "            remove batch padding from model outputs before returning\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            - 'predictions' (list of lists): first list is over datasets; second list is over\n",
    "              batches in the dataset; each element is a numpy array of the label probability\n",
    "              distribution\n",
    "            - 'weak_labels' (list of lists): corresponding weak labels\n",
    "            - 'labels' (list of lists): corresponding labels\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "\n",
    "        pad = self.hparams.get('sequence_pad', 0)\n",
    "\n",
    "\n",
    "        # initialize outputs dict\n",
    "        keys = ['y_logits','y_probs','y_sample','qz_xy_mean','qz_xy_logvar'\n",
    "                ,'pz_y_mean','pz_y_logvar','reconstruction']\n",
    "        \n",
    "        results_dict = {}\n",
    "        for key in keys:\n",
    "            results_dict[key] = [[] for _ in range(data_generator.n_datasets)]\n",
    "\n",
    "            for sess, dataset in enumerate(data_generator.datasets):\n",
    "                results_dict[key][sess] = [np.array([]) for _ in range(dataset.n_sequences)]\n",
    "                \n",
    "\n",
    "        # partially fill container (gap trials will be included as nans)\n",
    "        dtypes = ['train', 'val', 'test']\n",
    "        for dtype in dtypes:\n",
    "            data_generator.reset_iterators(dtype)\n",
    "            for i in range(data_generator.n_tot_batches[dtype]):\n",
    "                data, sess_list = data_generator.next_batch(dtype)\n",
    "                outputs_dict = self.forward(data['markers'])\n",
    "                # remove padding if necessary\n",
    "                if pad > 0 and remove_pad:\n",
    "                    for key, val in outputs_dict.items():\n",
    "                        outputs_dict[key] = val[:, pad:-pad] if val is not None else None\n",
    "                # loop over sequences in batch\n",
    "                for s, sess in enumerate(sess_list):\n",
    "                    batch_idx = data['batch_idx'][s].item()\n",
    "                    for key in keys:\n",
    "                        \n",
    "                        # push through log-softmax, since this is included in the loss and not model\n",
    "                        results_dict[key][sess][batch_idx] = \\\n",
    "                            softmax(outputs_dict[key][s]).cpu().detach().numpy()\n",
    "                    \n",
    "\n",
    "        return results_dict\n",
    "        \n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Construct the model using hparams.\"\"\"\n",
    "\n",
    "        # set random seeds for control over model initialization\n",
    "        rng_seed_model = self.hparams.get('rng_seed_model', 0)\n",
    "        torch.manual_seed(rng_seed_model)\n",
    "        np.random.seed(rng_seed_model)\n",
    "\n",
    "        # select backbone network\n",
    "        if self.hparams['backbone'].lower() == 'temporal-mlp':\n",
    "            from daart.models.temporalmlp import TemporalMLP as Module\n",
    "        elif self.hparams['backbone'].lower() == 'tcn':\n",
    "            raise NotImplementedError('deprecated; use dtcn instead')\n",
    "        elif self.hparams['backbone'].lower() == 'dtcn':\n",
    "            from daart.models.tcn import DilatedTCN as Module\n",
    "        elif self.hparams['backbone'].lower() in ['lstm', 'gru']:\n",
    "            from daart.models.rnn import RNN as Module\n",
    "        elif self.hparams['backbone'].lower() == 'tgm':\n",
    "            raise NotImplementedError\n",
    "            # from daart.models.tgm import TGM as Module\n",
    "        else:\n",
    "            raise ValueError('\"%s\" is not a valid backbone network' % self.hparams['backbone'])\n",
    "\n",
    "        n_total_classes = self.hparams['output_size'] + self.hparams['n_aug_classes']\n",
    "        self.hparams['n_total_classes'] = n_total_classes\n",
    "        # build label prior: p(y)\n",
    "        # prior prob for observed classes: 0.5 / n_observed_classes\n",
    "        # prior prob for unobserved classes: 0.5 / n_aug_classes\n",
    "        probs = 0.5 * np.ones((total_classes,))\n",
    "        probs[:hparams['output_size']] /= hparams['output_size']\n",
    "        probs[hparams['output_size']:] /= hparams['n_aug_classes']\n",
    "        assert np.sum(probs) == 1\n",
    "        self.model['py'] = Categorical(probs=probs)        \n",
    "        \n",
    "        # build classifier: q(y|x)\n",
    "        self.model['qy_x'] = Module(\n",
    "            self.hparams, \n",
    "            in_size=self.hparams['input_size'], \n",
    "            hid_size=self.hparams['n_hid_units'], \n",
    "            out_size=n_total_classes)\n",
    "        # add softmax to output\n",
    "       # name = 'softmax'\n",
    "       # self.model['classifier'].add_module(name, nn.Softmax(dim=2))\n",
    "        self.hparams['qy_x_temperature'] = 1\n",
    "        \n",
    "        # build encoder: q(z|x,y)\n",
    "        # for now we will concatenate x and y to infer z; perhaps in the future we\n",
    "        # can try a lookup table?\n",
    "        self.model['encoder'] = Module(\n",
    "            self.hparams, \n",
    "            in_size=n_total_classes + self.hparams['input_size'],\n",
    "            hid_size=self.hparams['n_hid_units'],\n",
    "            out_size=self.hparams['n_hid_units'])\n",
    "        self.model['qz_xy_mean'] = self._build_linear(\n",
    "            global_layer_num=len(self.model['classifier'].model), name='qz_xy_mean',\n",
    "            in_size=self.hparams['n_hid_units'], out_size=self.hparams['n_hid_units'])\n",
    "        self.model['qz_xy_logvar'] = self._build_linear(\n",
    "            global_layer_num=len(self.model['classifier'].model), name='qz_xy_logvar',\n",
    "            in_size=self.hparams['n_hid_units'], out_size=self.hparams['n_hid_units'])\n",
    "        \n",
    "        # build latent_generator: p(z|y)\n",
    "        # linear layer is essentially a lookup table of shape (n_hid_units, n_total_classes)\n",
    "        self.model['pz_y_mean'] = self._build_linear(\n",
    "            0, 'pz_y_mean', n_total_classes, self.hparams['n_hid_units'])\n",
    "        self.model['pz_y_logvar'] = self._build_linear(\n",
    "            0, 'pz_y_logvar', n_total_classes, self.hparams['n_hid_units'])\n",
    "        \n",
    "        # build decoder: p(x|z)\n",
    "        self.model['decoder'] = Module(\n",
    "            self.hparams, \n",
    "            in_size=self.hparams['n_hid_units'],\n",
    "            hid_size=self.hparams['n_hid_units'],\n",
    "            out_size=self.hparams['input_size'])\n",
    "        \n",
    "        self.hparams['kl_weight'] = 1  # weight in front of kl term; anneal this using callback\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"Process input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            observation data of shape (n_sequences, sequence_length, n_markers)\n",
    "        y : torch.Tensor\n",
    "            label data of shape (n_sequences, sequence_length)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict of model outputs/internals as torch tensors\n",
    "            - 'y_probs' (torch.Tensor): model classification\n",
    "               shape of (n_sequences, sequence_length, n_classes)\n",
    "            - 'y_sample' (torch.Tensor): sample from concrete distribution\n",
    "              shape of (n_sequences, sequence_length, n_classes)\n",
    "            - 'z_mean' (torch.Tensor): mean of appx posterior of latents in variational models\n",
    "              shape of (n_sequences, sequence_length, embedding_dim)\n",
    "            - 'z_logvar' (torch.Tensor): logvar of appx posterior of latents in variational models\n",
    "              shape of (n_sequences, sequence_length, embedding_dim)\n",
    "            - 'reconstruction' (torch.Tensor): input decoder prediction\n",
    "              shape of (n_sequences, sequence_length, n_markers)\n",
    "\n",
    "        \"\"\"\n",
    "        # push inputs through classifier to get q(y|x)\n",
    "        y_logits = self.model['qy_x'](x)\n",
    "        \n",
    "        # initialize and sample q(y|x) (should be a one-hot vector)\n",
    "        y_probs = nn.Softmax(dim=2)(y_logits)\n",
    "        qy_x = RelaxedOneHotCategorical(\n",
    "            temperature=self.hparams['qy_x_temperature'], probs=y_probs)\n",
    "        y_sample = qy_x.rsample()  # (n_sequences, sequence_length, n_total_classes)\n",
    "        \n",
    "        # concatenate sample with input x\n",
    "        # (n_sequences, sequence_length, n_total_classes)\n",
    "        y_onehot = make_one_hot(y, self.hparams['n_total_classes'])\n",
    "        \n",
    "        # init y_mixed, which will contain true labels for labeled data, samples for unlabled data\n",
    "        y_mixed = torch.copy(y_onehot)  # (n_sequences, sequence_length, n_total_classes)\n",
    "        # loop over sequences in batch\n",
    "        idxs_labeled = torch.zeros_like(y)\n",
    "        for s in y_mixed.shape[0]:\n",
    "            # for each sequence, update y_mixed with samples when true label is 0 \n",
    "            # (i.e. no label)\n",
    "            idxs_labeled[s] = y[s] != 0\n",
    "            y_mixed[s, ~idxs_labeled[s], :] = y_sample[s, ~idxs_labeled[s]]\n",
    "        \n",
    "        xy = torch.cat([x, y_mixed], dim=2)\n",
    "        \n",
    "        # push [y, x] through encoder to get parameters of q(z|x,y)\n",
    "        w = self.model['encoder'](xy)\n",
    "        qz_xy_mean = self.model['qz_xy_mean'](w)\n",
    "        qz_xy_logvar = self.model['qz_xy_logvar'](w)\n",
    "        qz_xy = MultivariateNormal(qz_xy_mean, torch.diagonal(torch.exp(qz_xy_logvar)))\n",
    "        z_xy_sample = qz_xy.rsample()\n",
    "        \n",
    "        # push y through generative model to get parameters of p(z|y)\n",
    "        pz_y_mean = self.model['pz_y_mean'](y_mixed)\n",
    "        pz_y_logvar = self.model['pz_y_logvar'](y_mixed)\n",
    "        \n",
    "        # push sampled z from through decoder to get reconstruction\n",
    "        # this will be the mean of p(x|z)\n",
    "        x_hat = self.model['decoder'](z_sample)\n",
    "\n",
    "        return {\n",
    "            'y_logits': y_logits, # (n_sequences, sequence_length, n_classes)\n",
    "            'y_probs': y_probs,  # (n_sequences, sequence_length, n_classes)\n",
    "            'y_sample': y_sample,  # (n_sequences, sequence_length, n_classes)\n",
    "            'qz_xy_mean': qz_xy_mean,  # (n_sequences, sequence_length, embedding_dim)\n",
    "            'qz_xy_logvar': qz_xy_logvar,  # (n_sequences, sequence_length, embedding_dim)\n",
    "            'pz_y_mean': pz_y_mean,  # (n_sequences, sequence_length, embedding_dim)\n",
    "            'pz_y_logvar': pz_y_logvar,  # (n_sequences, sequence_length, embedding_dim)\n",
    "            'qz_xy': qz_xy, # Multivariate Normal distribution object\n",
    "            'reconstruction': x_hat,  # (n_sequences, sequence_length, n_markers)\n",
    "            'idxs_labeled': idxs_labeled,  # (n_sequences, sequence_length)\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def predict_labels(self, data_generator, remove_pad=True, mode='eval'):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data_generator : DataGenerator object\n",
    "            data generator to serve data batches\n",
    "        remove_pad : bool\n",
    "            remove batch padding from model outputs before returning\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            - 'predictions' (list of lists): first list is over datasets; second list is over\n",
    "              batches in the dataset; each element is a numpy array of the label probability\n",
    "              distribution\n",
    "            - 'weak_labels' (list of lists): corresponding weak labels\n",
    "            - 'labels' (list of lists): corresponding labels\n",
    "        \"\"\"\n",
    "        if mode == 'eval':\n",
    "            self.eval()\n",
    "        else:\n",
    "            self.train()\n",
    "\n",
    "        \"\"\"\n",
    "          _______ ____  _____   ____  \n",
    "         |__   __/ __ \\|  __ \\ / __ \\ \n",
    "            | | | |  | | |  | | |  | |\n",
    "            | | | |  | | |  | | |  | |\n",
    "            | | | |__| | |__| | |__| |\n",
    "            |_|  \\____/|_____/ \\____/ \n",
    "            \n",
    "        \"\"\"\n",
    "        pad = self.hparams.get('sequence_pad', 0)\n",
    "\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # initialize containers\n",
    "\n",
    "        # softmax outputs\n",
    "        labels = [[] for _ in range(data_generator.n_datasets)]\n",
    "        # logits\n",
    "        scores = [[] for _ in range(data_generator.n_datasets)]\n",
    "        # latent representation\n",
    "        embedding = [[] for _ in range(data_generator.n_datasets)]\n",
    "        # predictions on regression task\n",
    "        task_predictions = [[] for _ in range(data_generator.n_datasets)]\n",
    "        for sess, dataset in enumerate(data_generator.datasets):\n",
    "            labels[sess] = [np.array([]) for _ in range(dataset.n_sequences)]\n",
    "            scores[sess] = [np.array([]) for _ in range(dataset.n_sequences)]\n",
    "            embedding[sess] = [np.array([]) for _ in range(dataset.n_sequences)]\n",
    "            task_predictions[sess] = [np.array([]) for _ in range(dataset.n_sequences)]\n",
    "\n",
    "        # partially fill container (gap trials will be included as nans)\n",
    "        dtypes = ['train', 'val', 'test']\n",
    "        for dtype in dtypes:\n",
    "            data_generator.reset_iterators(dtype)\n",
    "            for i in range(data_generator.n_tot_batches[dtype]):\n",
    "                data, sess_list = data_generator.next_batch(dtype)\n",
    "                outputs_dict = self.forward(data['markers'])\n",
    "                # remove padding if necessary\n",
    "                if pad > 0 and remove_pad:\n",
    "                    for key, val in outputs_dict.items():\n",
    "                        outputs_dict[key] = val[:, pad:-pad] if val is not None else None\n",
    "                # loop over sequences in batch\n",
    "                for s, sess in enumerate(sess_list):\n",
    "                    batch_idx = data['batch_idx'][s].item()\n",
    "                    # push through log-softmax, since this is included in the loss and not model\n",
    "                    labels[sess][batch_idx] = \\\n",
    "                        softmax(outputs_dict['labels'][s]).cpu().detach().numpy()\n",
    "                    embedding[sess][batch_idx] = \\\n",
    "                        outputs_dict['embedding'][s].cpu().detach().numpy()\n",
    "                    if return_scores:\n",
    "                        scores[sess][batch_idx] = \\\n",
    "                            outputs_dict['labels'][s].cpu().detach().numpy()\n",
    "                    if outputs_dict.get('task_prediction', None) is not None:\n",
    "                        task_predictions[sess][batch_idx] = \\\n",
    "                            outputs_dict['task_prediction'][s].cpu().detach().numpy()\n",
    "\n",
    "        return {\n",
    "            'labels': labels,\n",
    "            'scores': scores,\n",
    "            'embedding': embedding,\n",
    "            'task_predictions': task_predictions,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def training_step(self, data, accumulate_grad=True, **kwargs):\n",
    "        \"\"\"Calculate negative log-likelihood loss for supervised models.\n",
    "        The batch is split into chunks if larger than a hard-coded `chunk_size` to keep memory\n",
    "        requirements low; gradients are accumulated across all chunks before a gradient step is\n",
    "        taken.\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : dict\n",
    "            signals are of shape (n_sequences, sequence_length, n_channels)\n",
    "        accumulate_grad : bool, optional\n",
    "            accumulate gradient for training step\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            - 'loss' (float): total loss (negative log-like under specified noise dist)\n",
    "            - other loss terms depending on model hyperparameters\n",
    "        \"\"\"\n",
    "\n",
    "        # define hyperparams\n",
    "        lambda_strong = self.hparams.get('lambda_strong', 1)\n",
    "        kl_weight = self.hparams.get('kl_weight', 1)\n",
    "\n",
    "        # index padding for convolutions\n",
    "        pad = self.hparams.get('sequence_pad', 0)\n",
    "\n",
    "        # push data through model\n",
    "        markers_wpad = data['markers']\n",
    "        labels_wpad = data['labels_strong']\n",
    "        outputs_dict = self.forward(markers_wpad, labels_wpad)\n",
    "\n",
    "        # remove padding from supplied data\n",
    "        if pad > 0:\n",
    "            labels_strong = data['labels_strong'][:, pad:-pad, ...]\n",
    "        else:\n",
    "            labels_strong = data['labels_strong']\n",
    "        \n",
    "        # remove padding from model output\n",
    "        if pad > 0:\n",
    "            markers = markers_wpad[:, pad:-pad, ...]\n",
    "            # remove padding from model output\n",
    "            for key, val in outputs_dict.items():\n",
    "                outputs_dict[key] = val[:, pad:-pad, ...] if val is not None else None\n",
    "        else:\n",
    "            markers = markers_wpad\n",
    "            \n",
    "        # reshape everything to be (n_sequences * sequence_length, ...)\n",
    "        N = markers.shape[0] * markers.shape[1]\n",
    "        markers_rs = torch.reshape(markers, (N, markers.shape[-1]))\n",
    "        labels_rs = torch.reshape(labels_strong, (N,))\n",
    "        outputs_dict_rs = {}\n",
    "        for key, val in outputs_dict.items():\n",
    "            if isinstance(val, torch.Tensor):\n",
    "                outputs_dict_rs[key] = torch.reshape(val, (N, val.shape[-1]))\n",
    "            else:\n",
    "                outputs_dict_rs[key] = val\n",
    "                \n",
    "        # pull out indices of labeled data for loss computation\n",
    "        idxs_labeled = outputs_dict_rs['idxs_labeled']\n",
    "\n",
    "        # initialize loss to zero\n",
    "        loss = 0\n",
    "        loss_dict = {}\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # compute classification loss on labeled data\n",
    "        # ----------------------------------------------\n",
    "        if lambda_strong > 0:\n",
    "            loss_strong = self.class_loss(outputs_dict_rs['y_logits'], labels_rs)\n",
    "            loss += lambda_strong * loss_strong\n",
    "            # log\n",
    "            loss_dict['loss_classifier'] = loss_strong.item()\n",
    "            \n",
    "        # ------------------------------------\n",
    "        # compute reconstruction loss\n",
    "        # ------------------------------------ \n",
    "        # for now compute over both labeled and unlabeled data\n",
    "        reconstruction = outputs_dict_rs['reconstruction']\n",
    "        px_z_mean = reconstruction\n",
    "        px_z_var = torch.ones_like(px_z_mean)\n",
    "        px_z = MultivariateNormal(px_z_mean, torch.diagonal(px_z_var))\n",
    "        \n",
    "        loss_reconstruction = px_z.log_prob(markers_rs)\n",
    "        loss += loss_reconstruction\n",
    "        # log\n",
    "        loss_dict['loss_reconstruction'] = loss_reconstruction.item()\n",
    "        \n",
    "        \n",
    "        # ----------------------------------------\n",
    "        # compute kl divergence b/t qz_xy and pz_y\n",
    "        # ----------------------------------------   \n",
    "        # for now compute over both labeled and unlabeled data\n",
    "        qz_xy = outputs_dict['qz_xy']\n",
    "\n",
    "        pz_y_mean = outputs_dict['pz_y_mean']\n",
    "        pz_y_logvar = outputs_dict['pz_y_logvar']\n",
    "        pz_y = MultivariateNormal(pz_y_mean, torch.diagonal(torch.exp(pz_y_logvar)))\n",
    "\n",
    "        loss_kl = kl_divergence(qz_xy, pz_y)\n",
    "        loss -= kl_weight * loss_kl\n",
    "        # log\n",
    "        loss_dict['kl_weight'] = kl_weight\n",
    "        loss_dict['loss_kl'] = loss_kl.item()\n",
    "            \n",
    "        # ---------------------------------------\n",
    "        # entropy loss of qy_x on unlabeled data\n",
    "        # ---------------------------------------\n",
    "        y_probs = outputs_dict_rs['y_probs'][~idxs_labeled]\n",
    "        loss_entropy = Categorical(y_probs).entropy()\n",
    "        loss_unlabeled -= entropy loss\n",
    "        \n",
    "        # log\n",
    "        loss_dict['loss_unlabeled'] = loss_unlabeled.item()\n",
    "        \n",
    "        # adding unlabeled loss to total loss\n",
    "        loss += loss_unlabeled\n",
    "\n",
    "        if accumulate_grad:\n",
    "            loss.backward()\n",
    "\n",
    "        # collect loss vals\n",
    "        loss_dict['loss'] = loss.item()\n",
    "\n",
    "        return loss_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "tensor([4, 5, 6, 7])\n",
      "tensor([ 8,  9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = torch.tensor(np.arange(24).reshape(2, 3, 4))\n",
    "print(a[0, 0])\n",
    "print(a[0, 1])\n",
    "print(a[0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19],\n",
      "        [20, 21, 22, 23]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.reshape(a, (6, 4))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]])\n"
     ]
    }
   ],
   "source": [
    "l = torch.tensor(np.arange(10).reshape(5, 2))\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(l, (10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Generator contains 5 SingleDataset objects:\n",
      "2019_08_07_fly2\n",
      "    signals: ['markers', 'labels_weak', 'labels_strong']\n",
      "    transforms: OrderedDict([('markers', ZScore()), ('labels_weak', None), ('labels_strong', None)])\n",
      "    paths: OrderedDict([('markers', '/home/mattw/Dropbox/shared/segmentation-data/fly/markers/2019_08_07_fly2_labeled.h5'), ('labels_weak', '/home/mattw/Dropbox/shared/segmentation-data/fly/labels-heuristic/2019_08_07_fly2_labels.csv'), ('labels_strong', '/home/mattw/Dropbox/shared/segmentation-data/fly/labels-hand/2019_08_07_fly2_labels.csv')])\n",
      "2019_08_08_fly1\n",
      "    signals: ['markers', 'labels_weak', 'labels_strong']\n",
      "    transforms: OrderedDict([('markers', ZScore()), ('labels_weak', None), ('labels_strong', None)])\n",
      "    paths: OrderedDict([('markers', '/home/mattw/Dropbox/shared/segmentation-data/fly/markers/2019_08_08_fly1_labeled.h5'), ('labels_weak', '/home/mattw/Dropbox/shared/segmentation-data/fly/labels-heuristic/2019_08_08_fly1_labels.csv'), ('labels_strong', '/home/mattw/Dropbox/shared/segmentation-data/fly/labels-hand/2019_08_08_fly1_labels.csv')])\n",
      "2019_08_20_fly2\n",
      "    signals: ['markers', 'labels_weak', 'labels_strong']\n",
      "    transforms: OrderedDict([('markers', ZScore()), ('labels_weak', None), ('labels_strong', None)])\n",
      "    paths: OrderedDict([('markers', '/home/mattw/Dropbox/shared/segmentation-data/fly/markers/2019_08_20_fly2_labeled.h5'), ('labels_weak', '/home/mattw/Dropbox/shared/segmentation-data/fly/labels-heuristic/2019_08_20_fly2_labels.csv'), ('labels_strong', '/home/mattw/Dropbox/shared/segmentation-data/fly/labels-hand/2019_08_20_fly2_labels.csv')])\n",
      "2019_10_10_fly3\n",
      "    signals: ['markers', 'labels_weak', 'labels_strong']\n",
      "    transforms: OrderedDict([('markers', ZScore()), ('labels_weak', None), ('labels_strong', None)])\n",
      "    paths: OrderedDict([('markers', '/home/mattw/Dropbox/shared/segmentation-data/fly/markers/2019_10_10_fly3_labeled.h5'), ('labels_weak', '/home/mattw/Dropbox/shared/segmentation-data/fly/labels-heuristic/2019_10_10_fly3_labels.csv'), ('labels_strong', '/home/mattw/Dropbox/shared/segmentation-data/fly/labels-hand/2019_10_10_fly3_labels.csv')])\n",
      "2019_10_14_fly3\n",
      "    signals: ['markers', 'labels_weak', 'labels_strong']\n",
      "    transforms: OrderedDict([('markers', ZScore()), ('labels_weak', None), ('labels_strong', None)])\n",
      "    paths: OrderedDict([('markers', '/home/mattw/Dropbox/shared/segmentation-data/fly/markers/2019_10_14_fly3_labeled.h5'), ('labels_weak', '/home/mattw/Dropbox/shared/segmentation-data/fly/labels-heuristic/2019_10_14_fly3_labels.csv'), ('labels_strong', '/home/mattw/Dropbox/shared/segmentation-data/fly/labels-hand/2019_10_14_fly3_labels.csv')])\n",
      "\n",
      "\n",
      "DTCN architecture\n",
      "------------------------\n",
      "Encoder:\n",
      "    0: DilationBlock\n",
      "        0: Conv1d(16, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "        1: LeakyReLU(negative_slope=0.05)\n",
      "        2: Dropout2d(p=0.1, inplace=False)\n",
      "        3: Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "        4: LeakyReLU(negative_slope=0.05)\n",
      "        5: Dropout2d(p=0.1, inplace=False)\n",
      "        6: residual connection\n",
      "        7: LeakyReLU(negative_slope=0.05)\n",
      "\n",
      "    1: DilationBlock\n",
      "        0: Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(8,), dilation=(2,))\n",
      "        1: LeakyReLU(negative_slope=0.05)\n",
      "        2: Dropout2d(p=0.1, inplace=False)\n",
      "        3: Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(8,), dilation=(2,))\n",
      "        4: LeakyReLU(negative_slope=0.05)\n",
      "        5: Dropout2d(p=0.1, inplace=False)\n",
      "        6: residual connection\n",
      "        7: LeakyReLU(negative_slope=0.05)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# build data generator\n",
    "# -------------------------------------\n",
    "signals = []\n",
    "transforms = []\n",
    "paths = []\n",
    "\n",
    "handlers = []\n",
    "heuristics = []\n",
    "for expt_id in hparams['expt_ids']:\n",
    "\n",
    "    signals_curr = []\n",
    "    transforms_curr = []\n",
    "    paths_curr = []\n",
    "\n",
    "    # DLC markers or features (i.e. from simba)\n",
    "    input_type = hparams.get('input_type', 'markers')\n",
    "    markers_file = os.path.join(hparams['data_dir'], input_type, expt_id + '_labeled.h5')\n",
    "    if not os.path.exists(markers_file):\n",
    "        markers_file = os.path.join(hparams['data_dir'], input_type, expt_id + '_labeled.csv')\n",
    "    if not os.path.exists(markers_file):\n",
    "        markers_file = os.path.join(hparams['data_dir'], input_type, expt_id + '_labeled.npy')\n",
    "    if not os.path.exists(markers_file):\n",
    "        raise FileNotFoundError('could not find marker file for %s' % expt_id)\n",
    "    signals_curr.append('markers')\n",
    "    transforms_curr.append(ZScore())\n",
    "    paths_curr.append(markers_file)\n",
    "\n",
    "    # heuristic labels\n",
    "    if hparams.get('lambda_weak', 0) > 0:\n",
    "        heur_labels_file = os.path.join(\n",
    "            hparams['data_dir'], 'labels-heuristic', expt_id + '_labels.csv')\n",
    "        signals_curr.append('labels_weak')\n",
    "        transforms_curr.append(None)\n",
    "        paths_curr.append(heur_labels_file)\n",
    "\n",
    "    # hand labels\n",
    "    if hparams.get('lambda_strong', 0) > 0:\n",
    "        if expt_id not in hparams['expt_ids_to_keep']:\n",
    "            hand_labels_file = None\n",
    "        else:\n",
    "            hand_labels_file = os.path.join(\n",
    "                hparams['data_dir'], 'labels-hand', expt_id + '_labels.csv')\n",
    "            if not os.path.exists(hand_labels_file):\n",
    "                hand_labels_file = None\n",
    "        signals_curr.append('labels_strong')\n",
    "        transforms_curr.append(None)\n",
    "        paths_curr.append(hand_labels_file)\n",
    "\n",
    "    # define data generator signals\n",
    "    signals.append(signals_curr)\n",
    "    transforms.append(transforms_curr)\n",
    "    paths.append(paths_curr)\n",
    "    \n",
    "    # load data handler\n",
    "    handler = DataHandler(expt_id, base_path=os.path.join(hparams['data_dir']))\n",
    "    handler.load_hand_labels()\n",
    "    states = np.argmax(handler.hand_labels.vals, axis=1)\n",
    "    cutoff = int(np.floor(states.shape[0] / hparams['batch_size'])) * hparams['batch_size']\n",
    "    handler.load_heuristic_labels()\n",
    "    states_h = np.argmax(handler.heuristic_labels.vals, axis=1)\n",
    "    states_heuristic = states_h[:cutoff]   \n",
    "    handlers.append(handler)\n",
    "    heuristics.append(heuristics)\n",
    "\n",
    "# build data generator\n",
    "print('Loading data...')\n",
    "data_generator = DataGenerator(\n",
    "    hparams['expt_ids'], signals, transforms, paths, device=hparams['device'],\n",
    "    batch_size=hparams['batch_size'], sequence_length=hparams['sequence_length'],\n",
    "    trial_splits=hparams['trial_splits'],\n",
    "    train_frac=hparams['train_frac'], sequence_pad=hparams['sequence_pad'],\n",
    "    input_type=hparams.get('input_type', 'markers'))\n",
    "print(data_generator)\n",
    "\n",
    "# automatically compute input/output sizes from data\n",
    "hparams['input_size'] = data_generator.datasets[0].data['markers'][0].shape[1]\n",
    "\n",
    "# -------------------------------------\n",
    "# build model\n",
    "# -------------------------------------\n",
    "hparams['rng_seed_model'] = hparams['rng_seed_train']  # TODO: get rid of this\n",
    "torch.manual_seed(hparams.get('rng_seed_model', 0))\n",
    "if hparams['model_type'] == 'segmenter':\n",
    "    model = Segmenter(hparams)\n",
    "elif hparams['model_type'] == 'gmdgm':\n",
    "    model = GMDGM(hparams)\n",
    "model.to(hparams['device'])\n",
    "print(model)\n",
    "\n",
    "# -------------------------------------\n",
    "# set up training callbacks\n",
    "# -------------------------------------\n",
    "callbacks = []\n",
    "if hparams.get('semi_supervised_algo', 'none') == 'pseudo_labels':\n",
    "    from daart.callbacks import AnnealHparam, PseudoLabels\n",
    "    if model.hparams['lambda_weak'] == 0:\n",
    "        print('warning! use lambda_weak in model.yaml to weight pseudo label loss')\n",
    "    else:\n",
    "        callbacks.append(AnnealHparam(\n",
    "            hparams=model.hparams, key='lambda_weak', epoch_start=hparams['anneal_start'],\n",
    "            epoch_end=hparams['anneal_end']))\n",
    "        callbacks.append(PseudoLabels(\n",
    "            prob_threshold=hparams['prob_threshold'], epoch_start=hparams['anneal_start']))\n",
    "        # set min_epochs to when annealings ends\n",
    "        hparams['min_epochs'] = hparams['anneal_end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.58s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from daart.train import Logger, Trainer\n",
    "\n",
    "# define training params\n",
    "train_kwargs = {\n",
    "    'learning_rate': 1e-4,       # adam learning rate\n",
    "    'l2_reg': 0,                 # general l2 reg on parameters\n",
    "    'min_epochs': 10,            # minimum number of training epochs\n",
    "    'max_epochs': 200,          # maximum number of training epochs\n",
    "    'val_check_interval': 1,     # requency with which to log performance on val data\n",
    "    'rng_seed_train': 0,         # control order in which data are served to model\n",
    "    'enable_early_stop': False,  # True to use early stopping; False will use max_epochs\n",
    "    'early_stop_history': 10,    # epochs over which to average early stopping metric\n",
    "    'save_last_model': False,    # true to save out last (as well as best) model\n",
    "}\n",
    "\n",
    "# fit model!\n",
    "# Trainer(data_gen, save_path=model_save_path, **train_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
